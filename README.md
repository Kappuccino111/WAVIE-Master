# WAVIE-Master

**WAVIE : Wavelet Augmented Vision Intermediate Encodings** is a lightweight, highly generalizable deepfake detector that merges the strengths of two state-of-the-art pipelines—**RINE** and **Wavelet-CLIP**—to achieve robust performance across diverse GAN and diffusion-based generative models.

## Table of Contents

1. Motivation
2. Background: Core Concepts
   - RINE (Representation From Intermediate Encoder Blocks)
   - Wavelet-CLIP
1. WAVIE: High-Level Pipeline
2. Architecture & Implementation Details
3. Repository Structure
4. Getting Started
5. Citation
---
## Motivation

Current video deepfake detectors often excel at spotting fakes generated by a single model or dataset but **fail to generalize** across different GAN and diffusion families. As generative techniques proliferate, there is an urgent need for a **unified**, **compute-efficient**, and **robust** detection pipeline that can handle:

- **Diverse synthetic artefacts:** GAN textures, diffusion noise patterns, colour/lighting inconsistencies
- **Generator-agnostic signals:** Artefacts that persist regardless of architecture or training data
- **Practical constraints:** Single-GPU training, minimal fine-tuning budget, rapid convergence

**WAVIE** addresses these by blending the multi-scale feature attention of RINE with the frequency-aware refinement of Wavelet-CLIP.

---

## Background: Core Concepts

### 1. RINE

**Representation From Intermediate Encoder Blocks (RINE)** repurposes CLIP-ViT internals to amplify low-level artefacts while preserving high-level semantics:

1. **Frozen CLIP-ViT backbone** (24 blocks) for feature extraction
2. **Harvest intermediate [CLS] tokens** from every transformer block to form a token stack `K ∈ ℝ^{b×n×d}`
3. **First projection (`Q₁`)**: Shallow MLP reduces token dimensionality to `d′` for each of the `n` blocks
4. **Trainable Importance Estimator (TIE)**: Learns per-block weights `α ∈ ℝ^{n×d′}`, softmax across blocks → weighted sum `K̃ ∈ ℝ^{b×d′}`
5. **Second projection (`Q₂`)** & **classification head**: MLP(s) to one logit for real/fake
6. **Dual-objective loss**: Binary cross-entropy + supervised contrastive loss for tighter separation

Key benefits:
- Leverages **layer-wise artefact cues** (low-level textures to high-level composition)
- **Adaptive weighting** highlights the most informative blocks
- **Frozen backbone** reduces trainable parameters and avoids catastrophic forgetting

### 2. Wavelet-CLIP

**Wavelet-CLIP** enhances the frozen CLIP embedding with frequency decomposition:

1. **Frozen CLIP-ViT** encodes an image into `Z ∈ ℝ^{768}`
2. **1D Discrete Wavelet Transform (DWT)** splits `Z` into low (`Z_low`) and high-frequency (`Z_high`) components
3. **Low-band MLP**: Learns to adjust `Z_low → Z_low'` without disturbing high-frequency details
4. **Inverse DWT**: Recombines `[Z_low', Z_high] → Z_new ∈ ℝ^{768}` --> `Inverse- DWT(Z_new)`
5. **Classification head**: Small MLP maps `Z_new` to a single logit
6. **Binary cross-entropy** training on real vs. fake labels

Key benefits:
- **Separates global statistics** (low-band) from fine noise patterns (high-band)
- **Surgical refinement** of broad cues preserves generator-specific artefacts
- **Minimal trainable parameters** → fast training, strong generalization

---

## WAVIE: High-Level Pipeline

WAVIE merges RINE’s multi-block attention with Wavelet-CLIP’s frequency-aware refinement:

| Stage | Source            | Operation                                                                                      |
|-------|-------------------|------------------------------------------------------------------------------------------------|
| 1     | Both              | Input image → frozen CLIP-ViT (all 24 blocks)                                                  |
| 2     | RINE              | Extract `[CLS]` from each block → stack into `K ∈ ℝ^{b×n×d}`                                      |
| 3     | RINE              | First projection MLP (`Q₁`): `K → K_q ∈ ℝ^{b×n×d′}`                                              |
| 4     | RINE              | TIE: learn `α ∈ ℝ^{n×d′}`, softmax over layers → `K̃ = Σ_l Softmax(α)_l ⊙ K_q[l] ∈ ℝ^{b×d′}`      |
| 5     | Wavelet-CLIP      | DWT on `K̃` → `[K_low, K_high]`; apply tiny MLP on `K_low → K_low'`                                |
| 6     | Wavelet-CLIP      | IDWT: recombine `[K_low', K_high] → K̂ ∈ ℝ^{b×d′}`                                               |
| 7     | RINE (modified)   | Second projection MLP (`Q₂`) + 2–3-layer head → single logit                                     |
| 8     | RINE              | Dual loss: `BCE + SupConLoss` on normalized features                                             |
| 9     | Both              | Inference: all CLIP weights + wavelet filters frozen; only small MLPs & TIE weights trainable   |

---

## Architecture & Implementation Details

1. **Feature Extraction** (_CLIP-ViT frozen_)  
2. **CLS Token Harvesting** (_hooks into each block_)  
3. **Q₁ MLP** (_per-block projection to `d′`_)  
4. **TIE** (_layer-importance weights + softmax + weighted sum_)  
5. **Wavelet DWT & Low-Band MLP** (_frequency split + refine_)  
6. **Wavelet IDWT** (_re-synthesis of refined features_)  
7. **Q₂ & Classification Head** (_final logit_)

---

WAVIE's blend of RINE’s multi-layer CLS fusion and Wavelet-CLIP’s low-band refinement is tailor-made to boost **generalisation** across wildly different fake generators. Here’s why we think it should outperform the usual “one-trick” deepfake detectors:

1. **Rich, multi-scale cues**
   * RINE harvests features from every CLIP-ViT block, so you get both low-level (colours, textures) and high-level (semantics) signals in one vector.
   * That breadth is critical when one generator leaves one kind of fingerprint (e.g. colour tone) and another leaves a completely different one (e.g. subtle geometric quirks).

2. **Frequency-aware surgery**
   * Wavelet-CLIP teaches you exactly which coarse statistics are tampered with by GANs or diffusion models—and leaves the fine-detail band alone, so you don’t erase the micro-artifacts that another model family might imprint.
   * By only re-learning the low-frequency part, you avoid over-fitting to any single fake type.

3. **Contrastive + BCE training**
   * RINE’s dual loss pulls all real videos close together and all fakes (regardless of source) into a compact “fake” cluster—so a new generator you’ve never seen still lands near the fake centroid.

4. **Frozen backbone, tiny heads**
   * You train only a few million parameters in under an hour on one GPU. That makes it nearly impossible to over-specialize on one dataset or model family.

